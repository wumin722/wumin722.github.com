---
layout: post
title: k均值聚类
summary:  
categories: statistic
tags: kmeans
publish: true
---
# {{ page.title }} #
{{ page.summary }} 

k-means算法是一种用于聚类或者说数据分段的启发式算法，它属于无监督学习的一种。聚类方法分为自上而下和自下而上两种方法，前者的算法是先把所有样本视为一类，然后不断从这个大类中分离出小类，直到不能再分为止；后者则相反，首先所有样本自成一类，然后不断两两合并，直到最终形成几个大类。
## 基本描述
k-means聚类的目标是使样本点到其质心（即均值）的距离平方和最小（也可以称之为组内平方和within-cluster sum of squares）。通常使用欧拉距离作为距离的度量标准。为了使每个特征在计算距离时会得到公平的待遇，我们需要对每个特征进行标准化（是其均值为0，标准差为1）。给定样本(x1,x2,..,xn),以及xi所属的类别c(i)。
其畸变函数（distort function）为：

$$
J(c,\mu)=\sum\_{i}^m ||x\_{i}-\mu\_{c(i)}||^2
$$

## 具体算法
### 选取初始均值点
常用的初始化方法有Forgy方法和随机划分（Random Partition）。Forgy方法随机抽取K个样本作为初始均值点;随机划分则将每个样本点随机划分类别，再计算每个类别的均值点作为初始均值点。Forgy方法有将初始均值点分开的倾向，而随机划分的均值点更靠近样本数据的中心点。对于标准的k-means算法和EM算法，一般使用Forgy方法更好。
### 迭代计算
* 确定样本点类别。将每个样本分配给离它最近的均值点
* 重新计算各类别均值点。均值点是由所属子样本每个变量的均值所组成的向量
重复迭代直到质心不变或者变化很小。k-means并不能保证达到全局最小值，所以应该选出不同的初始值多跑几遍。

## 与其他算法关系
 _the element of statistic_ 一书中将k-means、Learning Vector Quantization、以及高斯混合模型（Gaussian Mixtures）归类为原型方法（Prototype Methods）。
 
> Prototype Methods可用于分类，其要在特征空间中为每个类找到一些具有代表性的点，注意这些选出的prototype不一定是样本中的点。选出的每个prototype都会label为某一特定的类，对于一个新来的样本点，找到和距离最近的那个prototype，然后用这个prototype的类去标记。

k-means和EM算法的思想类似。调整样本点类别相当与EM算法中的E步，重新计算质心相当与M步。假设当前J没有达到最小值,那么
（E步）首先可以固定每个类的质心μj或者分布,调整每个样例的所属的类别c(i)或者确定属于每个类别的概率（称为权重也行） 来让J函数减少
（M步）接着固定c(i)或者权重 ,调整每个类的质心μj或者分布，也可以使J减小。
这两个过程就是内循环中使J单调递减的过程。当J递减到最小时,μ和c也同时收敛。
k-means和EM算法主要的区别在于一个直接制定样本所属类别，一个给予的是概率。所以k-means称为硬聚类（hard clustering），EM为软聚类（soft clustering）。

## R示例

```r
x <- rbind(matrix(rnorm(100, sd = 0.4), ncol = 2), matrix(rnorm(100, mean = 1, 
    sd = 0.4), ncol = 2))
colnames(x) <- c("x", "y")
(cl <- kmeans(x, 2))
```

```
## K-means clustering with 2 clusters of sizes 51, 49
## 
## Cluster means:
##          x        y
## 1 -0.04232 -0.01753
## 2  0.94569  1.08330
## 
## Clustering vector:
##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2
##  [71] 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1] 13.92 16.04
##  (between_SS / total_SS =  64.6 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"    
## [5] "tot.withinss" "betweenss"    "size"
```

```r
plot(x, col = cl$cluster, pch = 20, cex = 2)
points(cl$centers, col = 1:2, pch = "*", cex = 5)
```

![plot of chunk unnamed-chunk-1](/images/unnamed-chunk-1.png) 


