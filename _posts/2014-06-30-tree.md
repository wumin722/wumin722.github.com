---
layout: post
title: 决策树
summary: 用决策树分类
categories: [statistic]
tags: [tree]
publish: true
---
# {{ page.title }} #
{{ page.summary }} 


本文是参照约翰霍普金斯大学实用机器学习Predicting with tree一课所做的练习
展示CART（class and regression tree）的使用。R中的rpart包提供了这一功能。而
本文将通过caret包调用rpart对iris数据建立分类树

分类树简介
------------------------------

CART既能建立回归树也能建立分类树，但两者在实现上有一定区别。回归树的目标函数是残差平方和最小，而分数树力求不纯度（impurity）最小。不纯度一般使用分类误差率（Misclassification error）、信息熵（Imformation entropy）和基尼系
（Gini index）来衡量。
对于某一节点\(m\)，代表了区\(R\_{m}\)，其中的样本数量为\(N\_{m}\)。定义
$$
p(k)=\frac{1}{N\_{m}} \sum\_{R\_{m}} I(y\_{i}=k)
$$
三种不同的纯度函数为
$$
i(m)=1-p(k\{m}); k\_{m}=argmax\_{k}p(k)
$$

$$
i(m)=-\sum\_{k=1}^K p(k)logp(k)
$$

$$
i(m)=\sum\_{k=1}^K p(k)(1-p(k))
$$
在选择划分点时，选取信息增益（IG）最大的点。
$$
IG=i(m)-p\_{L}i(m\_{L})-p\_{R}i(m\_{R})
$$
\(m\_{L}\)和\(m\_{R}\)为划分后，节点m下面的左边和右边的两个子节点。\(p\_{L}\)和\( p\_{R} \)则为各子节点样本容量占母节点m样本容量的比例。
虽然基本理论已经清楚了，但如果不实践一下，很难对算法本省有清楚的了解。
针对iris数据，写了一个分类树算法。用Gini index 度量纯度，并且只使用于数值型的predictor。代码在本人的github上 https://github.com/wumin722/wumin722.github.com/blob/master/code/myClassifyTree.R

实例演示
------------------------------

```r
#### 加载程序包和数据集
require(rattle)
require(caret)
data(iris)

#### 按种类统计鸢尾花数据
table(iris$Species)
```

```
## 
##     setosa versicolor  virginica 
##         50         50         50
```

```r

#### 设置训练集和预测集
set.seed(123)
trainIndex <- createDataPartition(iris$Species, p = 0.8, list = F)
training <- iris[trainIndex, ]
testing <- iris[-trainIndex, ]
# dim(training)
```


散点图描述Sepal.Length和Petal.Length

```r
qplot(Sepal.Length, Petal.Length, colour = Species, data = training)
```

![plot of chunk unnamed-chunk-2](/images/tree-2.png) 

建立模型

```r
#### 用rpart包建立分类树模型
model <- train(Species ~ ., method = "rpart", data = training)
```

```
## Loading required package: rpart
## Loading required package: class
```

```r
#### 用本人所写的分类树函数
source("./myClassifyTree-copy.R")
model1 <- MyClassifyTree(dat = training[, c(5, 1:4)], max_nodes_num = 10, min_inpurity = 0.2)
#### 估计预测误差
predict_table <- table(testing$Species == predict(model, testing))
predict_table[[2]]/length(testing[, 1])
```

```
## [1] 0.9
```

```r

#### 查看决策树的规则
model$finalModel
```

```
## n= 120 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 120 80 setosa (0.33333 0.33333 0.33333)  
##   2) Petal.Length< 2.45 40  0 setosa (1.00000 0.00000 0.00000) *
##   3) Petal.Length>=2.45 80 40 versicolor (0.00000 0.50000 0.50000)  
##     6) Petal.Width< 1.65 41  2 versicolor (0.00000 0.95122 0.04878) *
##     7) Petal.Width>=1.65 39  1 virginica (0.00000 0.02564 0.97436) *
```

```r
#### MyClassifyTree结果
model1[, -1]
```

```
##      path                                       
## [1,] "root"                                     
## [2,] "root Petal.Length < 3"                    
## [3,] "root Petal.Length >= 3"                   
## [4,] "root Petal.Length >= 3 Petal.Width < 1.7" 
## [5,] "root Petal.Length >= 3 Petal.Width >= 1.7"
##      tab                                    is_leaf
## [1,] "setosa 40 versicolor 40 virginica 40" "FALSE"
## [2,] "setosa 40 versicolor 0 virginica 0"   TRUE   
## [3,] "setosa 0 versicolor 40 virginica 40"  FALSE  
## [4,] "setosa 0 versicolor 39 virginica 2"   TRUE   
## [5,] "setosa 0 versicolor 1 virginica 38"   TRUE
```

为尽量简单，MyClassifyTree函数只返回分类的规则以及每个节点中样本的构成。从结果来看，MyClassifyTree的规则与rpart的接近。区别在于我所用每个节点中样本点作为可能的划分点。而rpart则不是。rpart结果中的第一个节点为`Petal.Length = 2.45`。`MyClassifyTree`则为`Petal.Length = 3`。

```r
training[which(training$Petal.Length == 2.45), ]
```

```
## [1] Sepal.Length Sepal.Width  Petal.Length Petal.Width  Species     
## <0 rows> (or 0-length row.names)
```

```r
training[which(training$Petal.Length == 3), ]
```

```
##    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
## 99          5.1         2.5            3         1.1 versicolor
```



```r
# leaf 1
table(training[which(training$Petal.Length < 3), 5]) == table(training[which(training$Petal.Length < 
    2.45), 5])
```

```
## 
##     setosa versicolor  virginica 
##       TRUE       TRUE       TRUE
```

```r
# leaf 2 training[which((training$Petal.Length > 3) &
# (training$Petal.Width > 1.65)),]
with(training, table(training[which(Petal.Length >= 2.45 & Petal.Width < 1.65), 
    5]))
```

```
## 
##     setosa versicolor  virginica 
##          0         39          2
```

```r
model1[4, 3]
```

```
## $tab
## [1] "setosa 0 versicolor 39 virginica 2"
```

```r
# leaf 3
with(training, table(training[which(Petal.Length >= 2.45 & Petal.Width >= 1.65), 
    5]))
```

```
## 
##     setosa versicolor  virginica 
##          0          1         38
```

```r
model1[5, 3]
```

```
## $tab
## [1] "setosa 0 versicolor 1 virginica 38"
```

虽然分割的点略有不同，但划分的结果都是一样的。

用rattle包所带的函数画图

```r
fancyRpartPlot(model$finalModel)
```

```
## Loading required package: rpart.plot
## Loading required package: RColorBrewer
```

![plot of chunk unnamed-chunk-4](/images/tree-4.png) 



